import streamlit as st

# Page configuration MUST be first
st.set_page_config(
    page_title="PTSD Emotion Recognition",
    page_icon="üé≠",
    layout="wide"
)

import torch
import numpy as np
import librosa
import matplotlib.pyplot as plt
import pandas as pd
import plotly.express as px
import tempfile
import os
import wave
import pyaudio
from datetime import datetime
import time
import sys
import torch.nn as nn

# Add the utils directory to Python path to import your modules
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))

# Import your actual emotion detection model
try:
    from emotion_detection import EmotionDetector, predict_emotion_live
    from audio_processing import AudioProcessor
    MODEL_LOADED = True
except ImportError as e:
    st.warning(f"Could not import emotion detection modules: {e}")
    MODEL_LOADED = False

class RealTimeRecorder:
    def __init__(self):
        self.audio = pyaudio.PyAudio()
        self.is_recording = False
        self.frames = []
    
    def record_audio(self, duration=5, sample_rate=16000, channels=1, chunk_size=1024):
        """Record audio for specified duration"""
        self.frames = []
        
        try:
            stream = self.audio.open(
                format=pyaudio.paInt16,
                channels=channels,
                rate=sample_rate,
                input=True,
                frames_per_buffer=chunk_size
            )
            
            st.info(f"üéôÔ∏è Recording for {duration} seconds... Speak now!")
            
            # Calculate chunks needed
            chunks_needed = int((sample_rate / chunk_size) * duration)
            
            # Record audio
            for i in range(chunks_needed):
                data = stream.read(chunk_size)
                self.frames.append(data)
            
            stream.stop_stream()
            stream.close()
            
            # Save to temporary file
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
            self._save_wav(temp_file.name, sample_rate, channels)
            
            st.success("‚úÖ Recording completed!")
            return temp_file.name
            
        except Exception as e:
            st.error(f"‚ùå Recording failed: {e}")
            return None
    
    def _save_wav(self, filename, sample_rate, channels):
        """Save recorded frames to WAV file"""
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(channels)
            wf.setsampwidth(self.audio.get_sample_size(pyaudio.paInt16))
            wf.setframerate(sample_rate)
            wf.writeframes(b''.join(self.frames))
    
    def close(self):
        """Clean up"""
        self.audio.terminate()

class PTSDStreamlitApp:
    def __init__(self):
        self.recorder = RealTimeRecorder()
        self.setup_models()
    
    def setup_models(self):
        """Setup models - initialize your actual models here"""
        st.session_state.setdefault('session_history', [])
        
        # Initialize your emotion detection model
        if MODEL_LOADED:
            try:
                # Initialize your model
                self.emotion_detector = EmotionDetector()
                st.sidebar.success("‚úÖ Emotion model loaded successfully")
            except Exception as e:
                st.sidebar.warning(f"‚ö†Ô∏è Could not initialize emotion model: {e}")
        else:
            st.sidebar.warning("‚ö†Ô∏è Using fallback analysis (demo mode)")
    
    def analyze_with_model(self, audio_path):
        """Analyze audio using your actual emotion detection model"""
        try:
            if not MODEL_LOADED:
                return self.fallback_analysis(audio_path)
            
            # Use your actual model for prediction
            result = predict_emotion_live(audio_path)
            
            if result:
                return result
            else:
                return self.fallback_analysis(audio_path)
                
        except Exception as e:
            st.error(f"‚ùå Model analysis failed: {e}")
            return self.fallback_analysis(audio_path)
    
    def fallback_analysis(self, audio_path):
        """Fallback analysis if model fails to load"""
        try:
            duration = librosa.get_duration(filename=audio_path)
            audio, sr = librosa.load(audio_path, sr=16000)
            rms = np.sqrt(np.mean(audio**2))
            
            emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']
            weights = np.random.dirichlet(np.ones(8))
            
            primary_emotion = emotions[np.argmax(weights)]
            confidence = np.max(weights)
            
            risk_factors = {
                'fearful': 0.8, 'angry': 0.7, 'sad': 0.5, 'disgust': 0.4,
                'surprised': 0.3, 'neutral': 0.2, 'calm': 0.1, 'happy': 0.0
            }
            
            risk_score = risk_factors.get(primary_emotion, 0.3)
            
            indicators = []
            if rms < 0.005:
                indicators.append("Low audio energy detected - possible emotional flatness")
                risk_score += 0.2
            
            return {
                'emotion': primary_emotion,
                'confidence': float(confidence),
                'all_probabilities': dict(zip(emotions, weights)),
                'ptsd_risk': min(risk_score, 1.0),
                'recommendation': self.get_recommendation(risk_score),
                'indicators': indicators,
                'audio_duration': duration,
                'audio_energy': rms
            }
            
        except Exception as e:
            st.error(f"‚ùå Fallback analysis error: {e}")
            return None

    def analyze_recorded_audio(self, audio_path):
        """Analyze the recorded audio using your actual model"""
        return self.analyze_with_model(audio_path)
    
    def get_recommendation(self, risk_score):
        """Get recommendation based on risk score"""
        if risk_score > 0.7:
            return "üö® HIGH ALERT: Strong PTSD indicators detected. Consider immediate therapist review."
        elif risk_score > 0.4:
            return "‚ö†Ô∏è MEDIUM ALERT: Moderate PTSD risk. Close monitoring recommended."
        else:
            return "‚úÖ LOW RISK: Emotional patterns within normal range. Continue monitoring."
    
    def display_results(self, result):
        """Display analysis results"""
        st.header("üìä Emotional Analysis Results")
        
        # Main results row
        col1, col2, col3 = st.columns(3)
        
        with col1:
            emotion_emoji = {
                'happy': 'üòä', 'sad': 'üò¢', 'angry': 'üò†', 
                'fearful': 'üò®', 'disgust': 'ü§¢', 'surprised': 'üò≤',
                'neutral': 'üòê', 'calm': 'üòå'
            }
            emoji = emotion_emoji.get(result['emotion'], 'üé≠')
            st.metric(
                "Primary Emotion", 
                f"{emoji} {result['emotion'].upper()}",
                f"{result['confidence']:.1%} confidence"
            )
        
        with col2:
            confidence_level = "High" if result['confidence'] > 0.7 else "Medium" if result['confidence'] > 0.5 else "Low"
            st.metric("Analysis Confidence", confidence_level)
        
        with col3:
            risk_score = result.get('ptsd_risk', 0)
            risk_level = "HIGH" if risk_score > 0.7 else "MEDIUM" if risk_score > 0.4 else "LOW"
            risk_color = "#ff6b6b" if risk_level == "HIGH" else "#ffd93d" if risk_level == "MEDIUM" else "#6bcf7f"
            
            st.markdown(f"""
            <div style="background-color: {risk_color}; padding: 15px; border-radius: 10px; color: {'white' if risk_level == 'HIGH' else 'black'};">
                <h3>PTSD Risk Assessment</h3>
                <p><strong>Level:</strong> {risk_level}</p>
                <p><strong>Score:</strong> {risk_score:.2f}/1.0</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Audio metrics
        st.subheader("üéµ Audio Analysis")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Audio Duration", f"{result.get('audio_duration', 0):.2f}s")
        with col2:
            st.metric("Speech Energy", f"{result.get('audio_energy', 0):.4f}")
        
        # Emotion distribution chart
        st.subheader("üéØ Emotion Probability Distribution")
        emotions = list(result['all_probabilities'].keys())
        probabilities = list(result['all_probabilities'].values())
        
        fig = px.bar(
            x=emotions, 
            y=probabilities,
            color=probabilities,
            color_continuous_scale='Viridis',
            labels={'x': 'Emotions', 'y': 'Probability'}
        )
        fig.update_layout(showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
        
        # PTSD Insights
        st.subheader("üß† PTSD Insights & Recommendations")
        
        if 'indicators' in result and result['indicators']:
            st.markdown("**Detected Indicators:**")
            for indicator in result['indicators']:
                st.write(f"‚Ä¢ {indicator}")
        
        st.markdown("**Clinical Recommendation:**")
        st.info(result.get('recommendation', 'Continue monitoring as usual.'))
    
    def audio_file_analysis(self):
        st.header("üìÅ Upload Audio File for Analysis")
        
        uploaded_file = st.file_uploader(
            "Upload an audio file for emotion analysis",
            type=['wav', 'mp3', 'm4a', 'flac']
        )
        
        if uploaded_file is not None:
            st.audio(uploaded_file, format='audio/wav')
            
            if st.button("üîç Analyze Emotional Content", type="primary"):
                with st.spinner("Analyzing emotional content..."):
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        audio_path = tmp_file.name
                    
                    try:
                        result = self.analyze_recorded_audio(audio_path)
                        if result:
                            self.display_results(result)
                    finally:
                        os.unlink(audio_path)
    
    def realtime_analysis(self):
        st.header("üé§ Real-time Audio Recording")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("""
            ### Live Emotion Detection Session
            
            Record speech in real-time and get instant emotional analysis with 
            PTSD risk assessment. Perfect for therapy sessions.
            
            **Instructions:**
            1. Ensure your microphone is connected and working
            2. Click 'Start Recording' below
            3. Speak naturally for the selected duration
            4. View instant emotional analysis and PTSD risk assessment
            """)
            
            recording_duration = st.slider(
                "Recording Duration (seconds)",
                min_value=3,
                max_value=15,
                value=7,
                help="Longer recordings (5-10 seconds) provide better analysis"
            )
            
            # Recording button
            if st.button("üéôÔ∏è Start Recording Session", type="primary", use_container_width=True):
                # Show recording in progress
                with st.spinner(f"üî¥ Recording audio for {recording_duration} seconds... Speak now!"):
                    # Actually record audio
                    audio_file = self.recorder.record_audio(duration=recording_duration)
                    
                    if audio_file:
                        # Display the recorded audio
                        st.audio(audio_file, format='audio/wav')
                        
                        # Analyze the recorded audio
                        result = self.analyze_recorded_audio(audio_file)
                        
                        if result:
                            self.display_results(result)
                        
                        # Cleanup
                        os.unlink(audio_file)
        
        with col2:
            st.markdown("### Session Controls")
            model_status = "‚úÖ Loaded" if MODEL_LOADED else "‚ö†Ô∏è Demo Mode"
            st.metric("Model Status", model_status)
            st.metric("Microphone", "Available")
            st.metric("Analysis Mode", "Real-time")
            
            st.markdown("### Tips for Best Results")
            st.info("""
            - Speak clearly and naturally
            - Avoid background noise
            - 5-10 seconds is ideal
            - Use a quiet environment
            """)
    
    def home_page(self):
        st.markdown("""
        ## Welcome to PTSD Speech Emotion Recognition System
        
        This system analyzes speech patterns to detect emotional states that may indicate PTSD symptoms.
        
        ### Features:
        - **Real-time Recording**: Live audio analysis during therapy sessions
        - **File Upload**: Analyze pre-recorded audio files
        - **PTSD Risk Assessment**: Identify potential trauma indicators
        - **Clinical Insights**: Actionable recommendations for therapists
        """)
    
    def how_it_works_page(self):
        st.markdown("""
        ## How It Works
        
        The system uses advanced AI to analyze vocal characteristics and detect emotional patterns
        associated with Post-Traumatic Stress Disorder.
        
        **Technology Stack:**
        - wav2vec2.0 for speech feature extraction
        - Deep learning for emotion classification
        - Real-time audio processing
        - PTSD risk assessment algorithms
        """)
    
    def main(self):
        """Main method that was missing"""
        st.title("üé≠ PTSD Speech Emotion Recognition System")
        
        # Sidebar navigation
        st.sidebar.title("Navigation")
        app_mode = st.sidebar.radio(
            "Choose Analysis Mode:",
            ["üè† Home", "üìÅ Upload Audio File", "üé§ Real-time Recording", "üìä How It Works"]
        )
        
        if app_mode == "üè† Home":
            self.home_page()
        elif app_mode == "üìÅ Upload Audio File":
            self.audio_file_analysis()
        elif app_mode == "üé§ Real-time Recording":
            self.realtime_analysis()
        elif app_mode == "üìä How It Works":
            self.how_it_works_page()
    
    def __del__(self):
        """Cleanup when app closes"""
        if hasattr(self, 'recorder'):
            self.recorder.close()

def main():
    app = PTSDStreamlitApp()
    app.main()

if __name__ == "__main__":
    main()